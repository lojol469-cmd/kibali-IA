# ğŸ¤– Configuration des ModÃ¨les IA - Kibali

## ğŸ“ Structure des ModÃ¨les

Tous les modÃ¨les IA utilisÃ©s par Kibali sont centralisÃ©s dans `kibali_data/models/`:

```
kibali_data/models/
â”œâ”€â”€ MODEL_PATHS.py          # Configuration centralisÃ©e (ce fichier)
â”œâ”€â”€ huggingface_cache/      # Symlink vers ~/.cache/huggingface/hub
â”œâ”€â”€ qwen2.5-1.5b/          # LLM local (Qwen 1.5B)
â”œâ”€â”€ clip/                   # Vision AI (CLIP)
â”œâ”€â”€ sentence-transformers/  # Embeddings pour RAG
â”œâ”€â”€ easyocr/               # ModÃ¨les OCR
â”œâ”€â”€ summarizer/            # RÃ©sumÃ© de texte (BART)
â”œâ”€â”€ translator/            # Traduction FR-EN
â”œâ”€â”€ ner/                   # Named Entity Recognition
â””â”€â”€ captioner/             # GÃ©nÃ©ration de lÃ©gendes d'images
```

## ğŸ¯ ModÃ¨les ConfigurÃ©s

### 1. LLM Local (Qwen 2.5-1.5B)
- **Nom:** `Qwen/Qwen2.5-1.5B-Instruct`
- **Licence:** Apache 2.0 âœ… Commercial
- **Taille:** ~3 GB
- **Usage:** Chat local sans API, gÃ©nÃ©ration de texte
- **Cache:** `kibali_data/models/qwen2.5-1.5b/`

### 2. Vision AI (CLIP)
- **Nom:** `openai/clip-vit-base-patch32`
- **Licence:** MIT âœ… Commercial
- **Taille:** ~600 MB
- **Usage:** Analyse sÃ©mantique d'images, classification
- **Cache:** `kibali_data/models/clip/`

### 3. Embeddings RAG (Sentence Transformers)
- **Nom:** `sentence-transformers/all-MiniLM-L6-v2`
- **Licence:** Apache 2.0 âœ… Commercial
- **Taille:** ~90 MB
- **Usage:** Vectorisation de documents pour recherche RAG
- **Cache:** `kibali_data/models/sentence-transformers/`

### 4. OCR (EasyOCR)
- **Langues:** FranÃ§ais + Anglais
- **Licence:** Apache 2.0 âœ… Commercial
- **Taille:** ~500 MB
- **Usage:** Extraction de texte depuis images
- **Cache:** `kibali_data/models/easyocr/`

### 5. Summarizer (BART)
- **Nom:** `facebook/bart-large-cnn`
- **Licence:** Apache 2.0 âœ… Commercial
- **Taille:** ~1.6 GB
- **Usage:** RÃ©sumÃ© automatique de textes longs
- **Cache:** `kibali_data/models/summarizer/`

### 6. Translator (Helsinki-NLP)
- **Nom:** `Helsinki-NLP/opus-mt-fr-en`
- **Licence:** Apache 2.0 âœ… Commercial
- **Taille:** ~300 MB
- **Usage:** Traduction franÃ§ais â†’ anglais
- **Cache:** `kibali_data/models/translator/`

### 7. NER (BERT)
- **Nom:** `dbmdz/bert-large-cased-finetuned-conll03-english`
- **Licence:** MIT âœ… Commercial
- **Taille:** ~1.3 GB
- **Usage:** Extraction d'entitÃ©s nommÃ©es (personnes, lieux, organisations)
- **Cache:** `kibali_data/models/ner/`

### 8. Captioner (BLIP)
- **Nom:** `Salesforce/blip-image-captioning-base`
- **Licence:** BSD-3-Clause âœ… Commercial
- **Taille:** ~1 GB
- **Usage:** GÃ©nÃ©ration de lÃ©gendes d'images
- **Cache:** `kibali_data/models/captioner/`

## ğŸ’¾ Espace Disque

- **Total estimÃ©:** ~8.4 GB
- **RecommandÃ©:** 15+ GB d'espace libre
- **Actuel:** 566 GB disponibles âœ…

## ğŸ”§ Utilisation

### Dans app.py

```python
from kibali_data.models.MODEL_PATHS import (
    QWEN_MODEL_NAME, QWEN_CACHE_DIR,
    CLIP_MODEL_NAME, CLIP_CACHE_DIR,
    SENTENCE_TRANSFORMER_MODEL, SENTENCE_TRANSFORMER_CACHE,
    ...
)

# Exemple: Charger CLIP
clip_model = CLIPModel.from_pretrained(
    CLIP_MODEL_NAME,
    cache_dir=str(CLIP_CACHE_DIR)
)
```

### Test de la configuration

```bash
python3.13 kibali_data/models/MODEL_PATHS.py
```

Affiche:
- ğŸ“¦ Liste de tous les modÃ¨les
- ğŸ“ Chemins de cache
- ğŸ’¾ Espace disque requis
- âœ… VÃ©rification de santÃ©

## ğŸ”„ Migration depuis ~/.cache

Par dÃ©faut, HuggingFace stocke les modÃ¨les dans `~/.cache/huggingface/hub`.

### Option 1: Symlink (Ã©conomie d'espace) âœ… RecommandÃ©

```bash
cd kibali_data/models
ln -s ~/.cache/huggingface/hub huggingface_cache
```

**Avantage:** Pas de duplication, utilise les modÃ¨les existants

### Option 2: Copie (indÃ©pendant)

```bash
cp -r ~/.cache/huggingface/hub/* kibali_data/models/huggingface_cache/
```

**Avantage:** Kibali autonome, portable

## ğŸ“ Maintenance

### VÃ©rifier les modÃ¨les installÃ©s

```python
from MODEL_PATHS import get_model_info, print_model_summary

print_model_summary()
```

### Nettoyer le cache

```bash
# Supprimer les anciens modÃ¨les non utilisÃ©s
cd kibali_data/models
du -sh */  # VÃ©rifier la taille
rm -rf <ancien_modele>/
```

### Mettre Ã  jour un modÃ¨le

```python
from transformers import AutoModel

# Le modÃ¨le sera automatiquement tÃ©lÃ©chargÃ© dans le bon dossier
model = AutoModel.from_pretrained(
    "nouveau/modele",
    cache_dir=str(MODELS_DIR / "nouveau_dossier")
)
```

## ğŸš€ Avantages de la Centralisation

âœ… **ClartÃ©:** Tous les chemins dans un seul fichier  
âœ… **PortabilitÃ©:** Facile de dÃ©placer Kibali  
âœ… **Maintenance:** Mise Ã  jour centralisÃ©e  
âœ… **Debugging:** Chemins explicites et traÃ§ables  
âœ… **Backup:** Un seul dossier Ã  sauvegarder  
âœ… **Licence:** Tous les modÃ¨les sont commercialement OK  

## ğŸ” SÃ©curitÃ©

- Les tokens HuggingFace restent dans `.env` (gitignored)
- Aucun modÃ¨le n'est commitÃ© sur Git
- Cache local = pas de fuite de donnÃ©es

## ğŸ“Š Performances

| ModÃ¨le | Chargement | RAM | VRAM (GPU) |
|--------|-----------|-----|------------|
| Qwen 1.5B | ~10s | 3 GB | 2 GB |
| CLIP | ~3s | 600 MB | 400 MB |
| Sentence-T | ~1s | 90 MB | 70 MB |
| EasyOCR | ~5s | 500 MB | 300 MB |

**Total:** ~4.2 GB RAM / ~3 GB VRAM

## ğŸ› DÃ©pannage

### Erreur: "Module MODEL_PATHS not found"

```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / "kibali_data" / "models"))
```

### Erreur: "No space left on device"

```bash
# VÃ©rifier l'espace
df -h

# Nettoyer le cache pip
pip cache purge

# Supprimer les anciens modÃ¨les
rm -rf kibali_data/models/ancien_modele/
```

### ModÃ¨le ne se charge pas

```python
# Forcer le re-tÃ©lÃ©chargement
import shutil
shutil.rmtree(QWEN_CACHE_DIR)  # Puis relancer l'app
```

## ğŸ“š RÃ©fÃ©rences

- HuggingFace Hub: https://huggingface.co/models
- Transformers Doc: https://huggingface.co/docs/transformers
- EasyOCR: https://github.com/JaidedAI/EasyOCR
- Sentence Transformers: https://www.sbert.net

---

**Auteur:** Kibali IA Team  
**Date:** DÃ©cembre 2025  
**Version:** 1.0  
**Licence:** Apache 2.0
