"""
Fonctionnalit√©s de KIbalione8 √† int√©grer dans kibali-IA
Ce fichier contient les syst√®mes avanc√©s :
1. M√©moire vectorielle chat pour continuit√© conversationnelle
2. Auto-apprentissage avec sous-mod√®les sklearn
3. Am√©lioration automatique de la DB via fouille internet
4. Highlighting et effets scintillants pour fluidit√©
"""

import os
import json
import time
import pickle
import numpy as np
import streamlit as st
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
import matplotlib.pyplot as plt
import re
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ===============================================
# 1. SYST√àME DE M√âMOIRE VECTORIELLE POUR HISTORIQUE CHAT
# ===============================================

def load_chat_vectordb(chat_vectordb_path, embedding_model):
    """Charger la base vectorielle pour l'historique chat"""
    if not os.path.exists(chat_vectordb_path):
        return None, "‚ö†Ô∏è Aucune base chat trouv√©e"
    
    try:
        from langchain_community.vectorstores import FAISS
        chat_vectordb = FAISS.load_local(
            chat_vectordb_path, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        return chat_vectordb, "‚úÖ Base chat charg√©e"
    except Exception as e:
        return None, f"‚ùå Erreur chat: {e}"

def add_to_chat_db(user_msg, ai_msg, chat_vectordb, chat_vectordb_path, embedding_model):
    """Ajouter un √©change user-AI √† la base chat"""
    from langchain_community.vectorstores import FAISS
    
    if chat_vectordb is None:
        chat_vectordb = FAISS.from_texts([""], embedding_model)
    
    exchange = f"User: {user_msg} ||| Assistant: {ai_msg}"
    doc = Document(
        page_content=exchange,
        metadata={"type": "chat_exchange", "timestamp": time.time()}
    )
    
    chat_vectordb.add_documents([doc])
    chat_vectordb.save_local(chat_vectordb_path)
    
    return chat_vectordb

def chat_rag_search(question, chat_vectordb, k=3):
    """Rechercher dans l'historique chat pour contexte"""
    if not chat_vectordb:
        return []
    
    try:
        return chat_vectordb.similarity_search(question, k=k)
    except Exception as e:
        print(f"‚ùå Erreur recherche chat: {e}")
        return []

# ===============================================
# 2. AUTO-APPRENTISSAGE AVEC SOUS-MOD√àLES SKLEARN
# ===============================================

def create_submodel_from_chat_history(chat_vectordb, submodels_path, submodel_type="classification"):
    """
    Cr√©e un petit sous-mod√®le sklearn √† partir de l'historique chat pour automatiser des r√©ponses.
    - Type: 'classification' pour classer les questions et pr√©dire des r√©ponses automatis√©es.
    Rend le mod√®le plus "humain" en apprenant des patterns conversationnels.
    """
    if not chat_vectordb:
        return None, "‚ùå Aucune base chat pour entra√Æner le sous-mod√®le"
   
    # Extraire les √©changes de l'historique
    exchanges = []
    try:
        for doc in list(chat_vectordb.docstore._dict.values()) or []:
            exchange = doc.page_content
            if "User:" in exchange and "Assistant:" in exchange:
                parts = exchange.split("|||")
                if len(parts) == 2:
                    user_part = parts[0].replace("User: ", "").strip()
                    ai_part = parts[1].replace("Assistant: ", "").strip()
                    exchanges.append((user_part, ai_part))
    except:
        return None, "‚ùå Erreur extraction √©changes"
   
    if len(exchanges) < 10:
        return None, f"‚ùå Historique chat trop court ({len(exchanges)} √©changes, min 10)"
   
    try:
        # Pr√©paration des donn√©es : TF-IDF pour vectorisation textuelle
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        X = vectorizer.fit_transform([user[0] for user in exchanges])
       
        # Pour classification simple (ex: pr√©dire si r√©ponse est informative ou autre)
        labels = []
        for user_msg, _ in exchanges:
            if re.search(r'\?', user_msg):
                labels.append(1)  # Question
            elif any(word in user_msg.lower() for word in ['info', 'savoir', 'expliquer']):
                labels.append(0)  # Info
            else:
                labels.append(2)  # Autre
       
        X_train, X_test, y_train, y_test = train_test_split(
            X, labels, test_size=0.2, random_state=42
        )
       
        if submodel_type == "classification":
            model = MultinomialNB()
        else:
            model = RandomForestClassifier(n_estimators=50)
       
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
       
        # Sauvegarder le mod√®le et vectorizer
        os.makedirs(submodels_path, exist_ok=True)
        model_path = os.path.join(
            submodels_path, 
            f"submodel_{submodel_type}_{int(time.time())}.pkl"
        )
        
        with open(model_path, 'wb') as f:
            pickle.dump({'model': model, 'vectorizer': vectorizer}, f)
       
        # Visualisation avec matplotlib : Accuracy plot
        fig, ax = plt.subplots()
        ax.bar(['Train', 'Test'], [1.0, accuracy])
        ax.set_title(f'Pr√©cision du sous-mod√®le {submodel_type.capitalize()}')
        ax.set_ylabel('Accuracy')
        ax.set_ylim([0, 1.1])
        
        plot_path = os.path.join(
            submodels_path,
            f"accuracy_plot_{submodel_type}_{int(time.time())}.png"
        )
        plt.savefig(plot_path)
        plt.close()
       
        return model_path, f"‚úÖ Sous-mod√®le {submodel_type} cr√©√© avec accuracy {accuracy:.2f}. Sauvegard√©: {model_path}"
    except Exception as e:
        return None, f"‚ùå Erreur cr√©ation sous-mod√®le: {e}"

def use_submodel_for_automation(query, submodel_path, submodels_path, submodel_type="classification"):
    """
    Utilise un sous-mod√®le pour automatiser une r√©ponse, rendant le comportement plus humain.
    """
    if not os.path.exists(submodel_path):
        return "‚ùå Sous-mod√®le non trouv√©"
   
    try:
        with open(submodel_path, 'rb') as f:
            data = pickle.load(f)
            model = data['model']
            vectorizer = data['vectorizer']
       
        query_vec = vectorizer.transform([query])
        prediction = model.predict(query_vec)[0]
       
        # R√©ponses automatis√©es bas√©es sur pr√©diction
        automated_responses = {
            0: "Voici des infos basiques sur ce sujet, bas√©es sur nos √©changes pass√©s.",
            1: "Bonne question ! Laisse-moi r√©fl√©chir √† √ßa en me basant sur ce qu'on a discut√© avant.",
            2: "Int√©ressant, je vais creuser un peu plus pour te r√©pondre de mani√®re personnalis√©e."
        }
       
        response = automated_responses.get(prediction, "R√©ponse automatis√©e g√©n√©r√©e.")
       
        # Visualisation: Distribution des features TF-IDF pour la query
        fig, ax = plt.subplots()
        tfidf_scores = query_vec.toarray()[0]
        top_features_idx = np.argsort(tfidf_scores)[-5:]
        top_scores = tfidf_scores[top_features_idx]
        
        feature_names = vectorizer.get_feature_names_out()
        top_feature_names = [feature_names[i] for i in top_features_idx]
        
        ax.bar(range(len(top_features_idx)), top_scores)
        ax.set_title('Top Features TF-IDF pour la Query')
        ax.set_xticks(range(len(top_features_idx)))
        ax.set_xticklabels(top_feature_names, rotation=45, ha='right')
        ax.tight_layout()
        
        plot_path = os.path.join(
            submodels_path,
            f"query_features_{int(time.time())}.png"
        )
        plt.savefig(plot_path)
        plt.close()
       
        return f"{response} (Pr√©diction: {prediction}) | Graph: {plot_path}"
    except Exception as e:
        return f"‚ùå Erreur utilisation sous-mod√®le: {e}"

# ===============================================
# 3. AM√âLIORATION DB PAR FOUILLE INTERNET
# ===============================================

def improve_database_with_web_search(
    topics, 
    num_results_per_topic, 
    vectordb, 
    vectordb_path, 
    embedding_model,
    enhanced_web_search_func,
    smart_content_extraction_func
):
    """
    Fouille internet sur des sujets sp√©cifiques et am√©liore la base de donn√©es.
    """
    from langchain_community.vectorstores import FAISS
    
    specific_topics = topics or [
        "p√©trole extraction techniques", 
        "topographie cartographie avanc√©e", 
        "sciences physiques m√©canique sol", 
        "sous-sol g√©ologie ressources",
        "ERT electrical resistivity tomography",
        "g√©ophysique m√©thodes prospection"
    ]
   
    if vectordb is None:
        vectordb = FAISS.from_texts([""], embedding_model)
   
    new_documents = []
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=100
    )
   
    for topic in specific_topics:
        print(f"üîç Fouille internet pour: {topic}")
        
        search_results = enhanced_web_search_func(
            topic, 
            max_results=num_results_per_topic, 
            search_type="both"
        )
       
        for result in search_results:
            content = f"Titre: {result.get('title', '')}\nContenu: {result.get('body', '')}\n"
            url = result.get('href') or result.get('url')
            
            if url and len(result.get('body', '')) < 500:
                extra_content = smart_content_extraction_func(url, max_length=2000)
                if "Impossible d'extraire" not in extra_content:
                    content += f"\nContenu d√©taill√©: {extra_content}"
           
            chunks = text_splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": url or topic,
                        "topic": topic,
                        "type": "web_enrichment",
                        "chunk_id": i,
                        "timestamp": time.time()
                    }
                )
                new_documents.append(doc)
   
    if new_documents:
        vectordb.add_documents(new_documents)
        vectordb.save_local(vectordb_path)
        return vectordb, f"‚úÖ Base am√©lior√©e: {len(new_documents)} nouveaux chunks ajout√©s sur {len(specific_topics)} sujets"
    else:
        return vectordb, "‚ö†Ô∏è Aucun nouveau contenu ajout√©"

# ===============================================
# 4. HIGHLIGHTING ET EFFETS SCINTILLANTS
# ===============================================

def highlight_important_words(text):
    """Met en √©vidence les mots importants avec effet scintillante et tooltip"""
    important_keywords = [
        'important', 'cl√©', 'essentiel', 'critique', 'principal', 
        'trajet', 'p√©trole', 'topographie', 'ERT', 'r√©sistivit√©',
        'g√©ophysique', 'analyse', 'donn√©es', 'graphique', 'tableau',
        'pr√©cis', 'd√©taill√©', 'complet'
    ]
    
    for keyword in important_keywords:
        pattern = rf'\b({keyword})\b'
        replacement = r'<span class="sparkle-word" title="\1: Terme cl√© pour la compr√©hension du contexte">\1</span>'
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    return text

def get_sparkle_css():
    """Retourne le CSS pour les effets scintillants"""
    return """
    /* Effet scintillante pour mots importants */
    .sparkle-word {
        color: #2196F3;
        background: linear-gradient(45deg, #2196F3, #21CBF3, #4ecdc4, #45b7d1);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: sparkle 2s linear infinite, gradient-shift 3s ease infinite;
        cursor: pointer;
        position: relative;
        padding: 2px 4px;
        border-radius: 4px;
        transition: transform 0.2s ease;
        font-weight: 700;
    }
    
    .sparkle-word:hover {
        transform: scale(1.1);
        text-shadow: 0 0 10px rgba(33, 150, 243, 0.8);
    }
    
    @keyframes sparkle {
        0%, 100% { text-shadow: 0 0 5px rgba(33, 150, 243, 0.5); }
        50% { text-shadow: 0 0 20px rgba(33, 150, 243, 1), 0 0 30px rgba(33, 203, 243, 0.7); }
    }
    
    @keyframes gradient-shift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    """

# ===============================================
# 5. GESTION AM√âLIOR√âE DU CHAT AVEC OUTILS
# ===============================================

def handle_chat_enhanced_with_tools(
    message, 
    history, 
    agent, 
    model_choice, 
    vectordb, 
    graph, 
    pois, 
    web_enabled,
    chat_vectordb,
    chat_vectordb_path,
    embedding_model,
    working_models,
    hybrid_search_enhanced_func,
    generate_answer_enhanced_func
):
    """
    Gestion am√©lior√©e du chat avec:
    - M√©moire vectorielle (historique)
    - Recherche hybride (locale + historique + web)
    - Auto-sauvegarde des √©changes
    """
    if not message.strip():
        return ""
    
    # Charger/cr√©er la base chat si n√©cessaire
    if chat_vectordb is None:
        from langchain_community.vectorstores import FAISS
        chat_vectordb = FAISS.from_texts([""], embedding_model)
    
    try:
        if not web_enabled:
            # Recherche hybride incluant historique chat
            docs = hybrid_search_enhanced_func(
                message, 
                vectordb, 
                k=3, 
                web_search_enabled=False,
                chat_vectordb=chat_vectordb
            )
            response = generate_answer_enhanced_func(
                message, 
                docs, 
                working_models[model_choice], 
                include_sources=True
            )
        else:
            # Utiliser l'agent avec tous les outils
            response = agent.run(message)
    
    except Exception as e:
        response = f"‚ùå Erreur: {e}\n\nTentative avec recherche locale..."
        try:
            docs = hybrid_search_enhanced_func(
                message, 
                vectordb, 
                k=3, 
                web_search_enabled=False,
                chat_vectordb=chat_vectordb
            )
            response = generate_answer_enhanced_func(
                message, 
                docs, 
                working_models[model_choice]
            )
        except Exception as e2:
            response = f"‚ùå Erreur compl√®te: {e2}"
    
    # Sauvegarder l'√©change dans la base chat
    chat_vectordb = add_to_chat_db(
        message, 
        response, 
        chat_vectordb,
        chat_vectordb_path,
        embedding_model
    )
    
    # Appliquer highlighting pour fluidit√©
    response = highlight_important_words(response)
    
    return response, chat_vectordb

# ===============================================
# 6. FONCTIONS D'INT√âGRATION DANS L'APP PRINCIPALE
# ===============================================

def init_kibalione8_systems(chatbot_dir):
    """
    Initialise tous les syst√®mes KibaliOne8 dans l'app principale.
    Retourne les chemins et configurations n√©cessaires.
    """
    # Cr√©er les dossiers n√©cessaires
    chat_vectordb_path = os.path.join(chatbot_dir, "chat_vectordb")
    submodels_path = os.path.join(chatbot_dir, "submodels")
    
    os.makedirs(chat_vectordb_path, exist_ok=True)
    os.makedirs(submodels_path, exist_ok=True)
    
    print("‚úÖ Syst√®mes KibaliOne8 initialis√©s:")
    print(f"   üìù Base chat: {chat_vectordb_path}")
    print(f"   üß† Sous-mod√®les: {submodels_path}")
    
    return {
        'chat_vectordb_path': chat_vectordb_path,
        'submodels_path': submodels_path
    }

def update_agent_with_chat_memory(
    model_choice, 
    vectordb, 
    graph, 
    pois, 
    chat_vectordb,
    working_models,
    create_enhanced_agent_func,
    get_cache_stats_func
):
    """
    Met √† jour l'agent en incluant la m√©moire chat.
    """
    model_name = working_models[model_choice]
    agent = create_enhanced_agent_func(model_name, vectordb, graph, pois, chat_vectordb)
    cache_info = get_cache_stats_func()
    return model_name, agent, cache_info
