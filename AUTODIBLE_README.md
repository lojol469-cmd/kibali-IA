# üéß Autodible - Assistant Audio Intelligent en Temps R√©el

## üéØ Qu'est-ce qu'Autodible ?

**Autodible** est un assistant audio intelligent qui vous aide **discr√®tement en temps r√©el** pendant vos conversations, r√©unions ou entretiens. Il √©coute via votre microphone et vous renvoie des suggestions **directement dans votre oreillette/casque**.

### ‚ö° Pipeline Ultra-Rapide

```
üé§ Micro ‚Üí Whisper (0.3s) ‚Üí Mistral-7B (0.8s) ‚Üí Coqui TTS (0.2s) ‚Üí üéß Oreillette
                            TOTAL: < 1.5 secondes
```

---

## üöÄ Caract√©ristiques

### ‚úÖ 100% Local & Priv√©
- **Aucune donn√©e envoy√©e sur internet**
- **Mod√®les locaux** : Mistral-7B-Instruct, Faster-Whisper, Coqui TTS
- **Pas de stockage** des conversations (par d√©faut)
- **Chiffrement** optionnel si sauvegarde activ√©e

### ‚ö° Ultra-Rapide
- **Latence cible : < 1.5s** (question ‚Üí suggestion oreillette)
- **Streaming LLM** : commence √† parler avant fin g√©n√©ration
- **Pipeline parall√®le** : STT + LLM + TTS en simultan√©
- **GPU optimis√©** : FP16, Flash Attention

### üéß Discret
- **Audio dans l'oreillette uniquement** (personne ne vous entend)
- **Voix chuchot√©e** configurable
- **Volume ajustable** en temps r√©el
- **Pas d'interface visible** (mode system tray)

### üß† Intelligent
- **5 modes d'assistance** : G√©n√©ral, R√©union Pro, Entretien, D√©bat, Apprentissage
- **Contexte conversationnel** : m√©morise les 3 derniers √©changes
- **Cache s√©mantique** : r√©ponses instantan√©es pour questions fr√©quentes
- **R√©ponses ultra-courtes** : 15-30 mots max (optimis√© pour l'oreille)

---

## üì¶ Installation

### 1. Installer les d√©pendances

```bash
cd /home/belikan/kibali-IA
pip install -r requirements_autodible.txt
```

### 2. V√©rifier la configuration

```bash
python autodible_config.py
```

Sortie attendue:
```
üéß Configuration Autodible
============================================================
  LLM Principal: mistralai/Mistral-7B-Instruct-v0.2
  STT Engine: faster-whisper
  TTS Engine: coqui-tts
  GPU Enabled: True
  Latence cible: 1.5s
  Mode privacy: 100% local
============================================================
‚úÖ Configuration Autodible valid√©e
```

### 3. (Optionnel) Optimisations GPU

Si vous avez un GPU NVIDIA avec CUDA :

```bash
# Flash Attention (acc√©l√©ration 3-5x)
pip install flash-attn --no-build-isolation

# PyTorch optimis√© CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

---

## üéÆ Utilisation

### Mode 1 : Interface Streamlit (Recommand√© pour d√©buter)

```bash
streamlit run autodible_ui.py
```

1. Ouvrez le navigateur sur `http://localhost:8501`
2. Choisissez votre **mode d'assistance**
3. Cliquez sur **"üöÄ D√©marrer Autodible"**
4. Mettez votre **casque/oreillette**
5. **Parlez** dans votre micro
6. **√âcoutez** les suggestions dans l'oreillette

### Mode 2 : Ligne de commande (Avanc√©)

```bash
python autodible.py
```

Sortie:
```
üéß AUTODIBLE - Assistant Audio Intelligent
============================================================
üöÄ Initialisation Autodible Engine...
üé§ AudioCapture initialis√©: 16000Hz, 1 canal(aux)
üó£Ô∏è Chargement Faster-Whisper (base)...
‚úÖ Faster-Whisper charg√© sur cuda
ü§ñ Chargement mistralai/Mistral-7B-Instruct-v0.2...
‚úÖ LLM charg√©: mistralai/Mistral-7B-Instruct-v0.2
üîä Chargement Coqui TTS...
‚úÖ Coqui TTS charg√©
‚úÖ Autodible Engine pr√™t!
‚ñ∂Ô∏è Pipeline temps r√©el d√©marr√©
‚úÖ Autodible actif - √âcoutez dans votre oreillette!

üí° Parlez dans votre micro, les suggestions arriveront dans votre oreillette
   Appuyez sur Ctrl+C pour arr√™ter
```

---

## üéØ Cas d'usage

### 1Ô∏è‚É£ Entretien d'embauche

**Sc√©nario:**
```
üó£Ô∏è Recruteur : "Parlez-moi de votre exp√©rience en IA"

üé§ [Autodible √©coute via votre micro]
ü§ñ [Analyse avec Mistral-7B]

üéß Dans VOTRE oreillette (0.8s apr√®s) :
   "Mentionne Kibali IA, int√©gration Mistral et Gemini,
    syst√®me de m√©moire vectorielle avec FAISS"

üó£Ô∏è Vous (avec confiance) : "J'ai d√©velopp√© Kibali IA, une plateforme..."
```

**Configuration:**
```python
# Dans autodible_ui.py
mode = "interview"  # Mode Entretien
```

### 2Ô∏è‚É£ R√©union professionnelle

**Sc√©nario:**
```
üó£Ô∏è Coll√®gue : "Quel est le ROI de ce projet ?"

üéß Autodible (oreillette) :
   "ROI estim√©: 35% sur 12 mois. Gains: automatisation 60%,
    r√©duction erreurs 40%"
```

**Configuration:**
```python
mode = "meeting"  # Mode R√©union Pro
```

### 3Ô∏è‚É£ N√©gociation commerciale

**Sc√©nario:**
```
üó£Ô∏è Client : "Votre concurrent propose 20% moins cher"

üéß Autodible :
   "Valeur ajout√©e: support 24/7, SLA 99.9%, formation incluse.
    Prix march√© moyen: +15% vs concurrent"
```

**Configuration:**
```python
mode = "debate"  # Mode D√©bat/N√©gociation
```

---

## ‚öôÔ∏è Configuration Avanc√©e

### Modifier le mod√®le LLM

**Option 1 : Changer vers Qwen (plus rapide)**

```python
# autodible_config.py
LLM_CONFIG["primary"]["model_name"] = "Qwen/Qwen2.5-1.5B-Instruct"
```

**Option 2 : Changer vers Phi-3 (√©quilibre)**

```python
LLM_CONFIG["primary"]["model_name"] = "microsoft/Phi-3-mini-4k-instruct"
```

### Ajuster la latence vs qualit√©

**Mode rapide (latence < 1s):**
```python
STT_CONFIG["model_size"] = "tiny"  # Whisper tiny
LLM_CONFIG["primary"]["max_new_tokens"] = 30  # R√©ponses plus courtes
LLM_CONFIG["primary"]["temperature"] = 0.2  # Plus d√©terministe
```

**Mode qualit√© (latence 1.5-2s):**
```python
STT_CONFIG["model_size"] = "small"  # Whisper small
LLM_CONFIG["primary"]["max_new_tokens"] = 60  # R√©ponses plus longues
LLM_CONFIG["primary"]["temperature"] = 0.4  # Plus cr√©atif
```

### Personnaliser la voix TTS

```python
TTS_CONFIG["speed"] = 1.0  # Vitesse normale (1.15 par d√©faut)
TTS_CONFIG["pitch"] = 1.0  # Pitch normal (0.95 par d√©faut)
TTS_CONFIG["volume"] = 0.5  # Volume plus bas
```

---

## üîß D√©pannage

### Probl√®me : "CUDA out of memory"

**Solution 1 : Quantification 8-bit**
```python
LLM_CONFIG["primary"]["load_in_8bit"] = True
```

**Solution 2 : Utiliser CPU**
```python
LLM_CONFIG["primary"]["device_map"] = "cpu"
```

**Solution 3 : Mod√®le plus petit**
```python
LLM_CONFIG["primary"]["model_name"] = "Qwen/Qwen2.5-1.5B-Instruct"
```

### Probl√®me : Latence trop √©lev√©e (> 3s)

**Diagnostic:**
```bash
python -c "
from autodible_config import SYSTEM_CONFIG
print(f'Latence cible: {SYSTEM_CONFIG[\"target_total_latency\"]}s')
"
```

**Solutions:**
1. Activer GPU si disponible
2. R√©duire `max_new_tokens` √† 30-40
3. Utiliser Whisper "tiny" au lieu de "base"
4. D√©sactiver `vad_filter` dans STT_CONFIG

### Probl√®me : Pas de son dans l'oreillette

**V√©rifier p√©riph√©riques audio:**
```python
import sounddevice as sd
print(sd.query_devices())
```

**Forcer device de sortie:**
```python
AUDIO_CONFIG["output_device"] = 2  # Remplacer par votre device ID
```

### Probl√®me : Transcription incorrecte

**Am√©liorer qualit√© STT:**
```python
STT_CONFIG["model_size"] = "small"  # ou "medium"
STT_CONFIG["beam_size"] = 5  # Plus pr√©cis (mais plus lent)
```

---

## üìä Performance

### Benchmarks (GPU NVIDIA RTX 3060)

| Composant | Mod√®le | Latence | Qualit√© |
|-----------|--------|---------|---------|
| **STT** | Faster-Whisper base | 0.2-0.3s | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **LLM** | Mistral-7B-Instruct | 0.5-0.8s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **TTS** | Coqui XTTS v2 | 0.2-0.3s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **TOTAL** | Pipeline complet | **0.9-1.4s** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Benchmarks (CPU Intel i7)

| Composant | Mod√®le | Latence | Qualit√© |
|-----------|--------|---------|---------|
| **STT** | Faster-Whisper base | 0.5-0.8s | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **LLM** | Qwen-1.5B-Instruct | 1.0-1.5s | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **TTS** | pyttsx3 | 0.1-0.2s | ‚≠ê‚≠ê‚≠ê |
| **TOTAL** | Pipeline complet | **1.6-2.5s** | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üõ°Ô∏è S√©curit√© & Privacy

### Donn√©es collect√©es : **AUCUNE**

- ‚ùå Pas d'enregistrement audio
- ‚ùå Pas de transcriptions sauvegard√©es
- ‚ùå Pas de connexion internet
- ‚úÖ 100% traitement local
- ‚úÖ Mod√®les charg√©s depuis cache local

### Activer sauvegarde (optionnel)

```python
# autodible_config.py
PRIVACY_CONFIG["save_transcripts"] = True  # Historique local
PRIVACY_CONFIG["encryption"] = True  # Chiffrer les sauvegardes
```

---

## üìù Logs

### Voir logs en temps r√©el

```bash
tail -f autodible.log
```

### Logs d√©taill√©s (debug)

```python
SYSTEM_CONFIG["log_level"] = "DEBUG"
```

---

## üöÄ Roadmap

### Version 1.1 (En cours)
- [ ] Support multi-langues (EN, ES, DE)
- [ ] Hotkey global (Ctrl+Shift+A)
- [ ] System tray icon
- [ ] Mode "Activation vocale" ("Aide-moi")

### Version 1.2
- [ ] Int√©gration recherche web temps r√©el (Tavily)
- [ ] Support WhisperX (encore plus rapide)
- [ ] Export conversations en markdown
- [ ] Profils utilisateur personnalis√©s

### Version 2.0
- [ ] Mode multi-utilisateurs (r√©union)
- [ ] D√©tection automatique du mode (ML)
- [ ] Support RAG avec documents utilisateur
- [ ] App mobile (contr√¥le √† distance)

---

## ü§ù Contribution

Autodible fait partie du projet **Kibali IA**.

**Repository:** https://github.com/lojol469-cmd/kibali-IA

---

## üìú Licence

Voir `LICENSE` dans le repository principal.

---

## üí° Support

**Questions ? Bugs ?**
- Cr√©er une issue sur GitHub
- Email : lojol469@gmail.com

---

## üéâ Remerciements

- **Mistral AI** - Mod√®le LLM Mistral-7B
- **OpenAI** - Whisper STT
- **Coqui.ai** - TTS XTTS v2
- **HuggingFace** - Infrastructure mod√®les

---

**Fait avec ‚ù§Ô∏è par lojol469-cmd**
